{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "filled-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stunning-meeting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number  1  has error of  0.1003919834265447\n",
      "epoch number  2  has error of  0.0560855360473059\n",
      "epoch number  3  has error of  0.04349345299223934\n",
      "epoch number  4  has error of  0.035102529303888824\n",
      "epoch number  5  has error of  0.029758929591297133\n",
      "epoch number  6  has error of  0.025320149060809833\n",
      "epoch number  7  has error of  0.02188188122431867\n",
      "epoch number  8  has error of  0.018974026016403526\n",
      "epoch number  9  has error of  0.016391486611742737\n",
      "epoch number  10  has error of  0.014257467574594711\n",
      "epoch number  11  has error of  0.012825755350200262\n",
      "epoch number  12  has error of  0.011537717682379281\n",
      "epoch number  13  has error of  0.01048421174482677\n",
      "epoch number  14  has error of  0.00962191166986355\n",
      "epoch number  15  has error of  0.009008987715978132\n",
      "epoch number  16  has error of  0.008441120323745843\n",
      "epoch number  17  has error of  0.007995692441003363\n",
      "epoch number  18  has error of  0.007499111624429259\n",
      "epoch number  19  has error of  0.007217985752638521\n",
      "epoch number  20  has error of  0.006935622594102633\n",
      "epoch number  21  has error of  0.006653579728074375\n",
      "epoch number  22  has error of  0.006363449757384261\n",
      "epoch number  23  has error of  0.006028399342396907\n",
      "epoch number  24  has error of  0.005899950596854717\n",
      "epoch number  25  has error of  0.005662410882184892\n",
      "epoch number  26  has error of  0.005554070367276775\n",
      "epoch number  27  has error of  0.005316974941803286\n",
      "epoch number  28  has error of  0.005025176201457504\n",
      "epoch number  29  has error of  0.004934564142274975\n",
      "epoch number  30  has error of  0.00484960606135366\n",
      "epoch number  31  has error of  0.004750192382584501\n",
      "epoch number  32  has error of  0.004557614644697973\n",
      "epoch number  33  has error of  0.004435288506456946\n",
      "epoch number  34  has error of  0.00443741823950202\n",
      "epoch number  35  has error of  0.0043191303626747045\n",
      "epoch number  36  has error of  0.004149893901654989\n",
      "epoch number  37  has error of  0.0041357399158539095\n",
      "epoch number  38  has error of  0.0041087916626421726\n",
      "epoch number  39  has error of  0.00397503242849054\n",
      "epoch number  40  has error of  0.003976853245830329\n",
      "epoch number  41  has error of  0.003905726179339645\n",
      "epoch number  42  has error of  0.0037620017070182283\n",
      "epoch number  43  has error of  0.0037183847261235943\n",
      "epoch number  44  has error of  0.0037043958210236376\n",
      "epoch number  45  has error of  0.003651834591473949\n",
      "epoch number  46  has error of  0.003657152343705434\n",
      "epoch number  47  has error of  0.0035867564303094777\n",
      "epoch number  48  has error of  0.003542143027896476\n",
      "epoch number  49  has error of  0.0034972802214827455\n",
      "epoch number  50  has error of  0.0034108732763940774\n",
      "epoch number  51  has error of  0.00338521208300785\n",
      "epoch number  52  has error of  0.0034124389585723286\n",
      "epoch number  53  has error of  0.0034009277974499184\n",
      "epoch number  54  has error of  0.003385723554888741\n",
      "epoch number  55  has error of  0.003348386660793224\n",
      "epoch number  56  has error of  0.0033553855704949375\n",
      "epoch number  57  has error of  0.0032844455271999174\n",
      "epoch number  58  has error of  0.0033201721532038933\n",
      "epoch number  59  has error of  0.0032848766662861946\n",
      "epoch number  60  has error of  0.003196789929582304\n",
      "epoch number  61  has error of  0.003185388895697584\n",
      "epoch number  62  has error of  0.003161550104773682\n",
      "epoch number  63  has error of  0.0031346812192261297\n",
      "epoch number  64  has error of  0.003077644151071678\n",
      "epoch number  65  has error of  0.0030315298381842944\n",
      "epoch number  66  has error of  0.0030256449142652122\n",
      "epoch number  67  has error of  0.003035619662796411\n",
      "epoch number  68  has error of  0.0030328786820012672\n",
      "epoch number  69  has error of  0.0029949329796704885\n",
      "epoch number  70  has error of  0.002940481569319096\n"
     ]
    }
   ],
   "source": [
    "# Activation Function\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "# Derivative of Activation Function\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "# Loss function, instead of ReLU and Sigmoid\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2));\n",
    "\n",
    "# Derivative of Loss Function\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size;\n",
    "\n",
    "# declare weights and biases\n",
    "w1 = np.random.rand(28*28*1, 100) - 0.5\n",
    "w2 = np.random.rand(100, 50) - 0.5\n",
    "w3 = np.random.rand(50, 10) - 0.5\n",
    "\n",
    "b1 = np.random.rand(1, 100)\n",
    "b2 = np.random.rand(1, 50)\n",
    "b3 = np.random.rand(1, 10)\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x = 5\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 20000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "samples = len(x_train[:2000])\n",
    "\n",
    "for x in range(70):\n",
    "    err = 0\n",
    "    for j in range(samples):\n",
    "        \n",
    "    # forward prop\n",
    "    # uses input as activation for z1, and propogates\n",
    "    # the output forwards for each activation\n",
    "        \n",
    "        input = x_train[j]\n",
    "        z1 = np.dot(input, w1) + b1\n",
    "        h1 = tanh(z1)\n",
    "        z2 = np.dot(h1, w2) + b2\n",
    "        h2 = tanh(z2)\n",
    "        z3 = np.dot(h2, w3) + b3\n",
    "        output = tanh(z3)\n",
    "        \n",
    "    # Calculate error of this sample\n",
    "        err += mse(y_train[j], output)\n",
    "        \n",
    "    # back propogation\n",
    "        \"\"\"calculate derivative of cost/w3, cost/w2, and cost/w1\n",
    "        to get errors for each weight and how much to tune them by.\n",
    "        Finally, subtract the error by each respective weight to make\n",
    "        the network more accurate.\"\"\"\n",
    "        error = mse_prime(y_train[j],output)\n",
    "        error_times_tanh_prime_z3 = error * tanh_prime(z3)\n",
    "        output_error = np.dot(h2.T, error_times_tanh_prime_z3)\n",
    "        j = np.dot(error_times_tanh_prime_z3, w3.T)\n",
    "        tanh_prime_z2_times_j = tanh_prime(z2) * j\n",
    "        h2_error = np.dot(h1.T,tanh_prime_z2_times_j)\n",
    "        k = np.dot(tanh_prime_z2_times_j, w2.T)\n",
    "        tanh_prime_z1_times_k = tanh_prime(z1) * k\n",
    "        h1_error = np.dot(input.T, tanh_prime_z1_times_k)\n",
    "        \n",
    "        w3 -= .15 * output_error\n",
    "        b3 -= .15 * error_times_tanh_prime_z3\n",
    "        w2 -= .15 * h2_error\n",
    "        b2 -= .15 * tanh_prime_z2_times_j\n",
    "        w1 -= .15 * h1_error\n",
    "        b1 -= .15 * tanh_prime_z1_times_k\n",
    "        \n",
    "    # Average out the error of this sample over all samples\n",
    "    # print it out for each epoch\n",
    "    err/=samples\n",
    "    print(\"epoch number \", x + 1, \" has error of \", err)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pacific-sterling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.00301005,  0.00403768, -0.00292536,  0.03586263, -0.01092165,\n",
      "         0.94970436, -0.01698962,  0.00683722, -0.01984634,  0.0134784 ]]), array([[ 9.90915692e-01,  9.23880808e-04, -2.26909574e-03,\n",
      "        -7.42488822e-04, -1.83033496e-03, -1.37660724e-03,\n",
      "        -1.14982696e-02,  1.13752209e-04, -9.32582668e-03,\n",
      "        -4.02667394e-04]]), array([[ 0.007277  ,  0.02123113, -0.0065323 , -0.01891555,  0.99073283,\n",
      "        -0.00849289, -0.01505545, -0.00123396, -0.00875344, -0.00126416]])]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "actual results: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"predict the output of input images through forward propogation\n",
    "and appending the result to a list. print out the output\n",
    "of neural network and true values.\"\"\"\n",
    "samples = len(x_train[0:3])\n",
    "result = []\n",
    "for x in range(samples):\n",
    "    input = x_train[x]\n",
    "    z1 = np.dot(input, w1) + b1\n",
    "    h1 = tanh(z1)\n",
    "    z2 = np.dot(h1, w2) + b2\n",
    "    h2 = tanh(z2)\n",
    "    z3 = np.dot(h2, w3) + b3\n",
    "    output = tanh(z3)\n",
    "    result.append(output)\n",
    "    \n",
    "print(result)\n",
    "print(\"\\n\\n\\n\")\n",
    "print('actual results: ')\n",
    "print(y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-posting",
   "metadata": {},
   "source": [
    "<h2>Next Steps</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-government",
   "metadata": {},
   "source": [
    "- Increase number of layers to compute the most accurate result\n",
    "- Experimenet with appropriate learning rate\n",
    "- Train on more data to get more accurate results\n",
    "- Use classes for the network and layers to more cleanly propogate forwards, backwards, and predict\n",
    "- Use different activation and loss functions to see how accuracy of output changes\n",
    "- Increase number of epochs\n",
    "- tweak starting biases and weights to get a smaller error quicker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-funds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
